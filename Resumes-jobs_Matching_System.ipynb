{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2370b460-2a41-4579-88f4-acfefbee086a",
   "metadata": {},
   "source": [
    "# Libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ea1317-d2b9-438b-b513-32edcb914be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.9/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.0/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 12.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 10.2 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc311939-edaf-47f5-bedb-adb85d4be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.training import Example\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8b0c02-392b-4c5b-9b17-8d6eaedafd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlapping_entities(data):\n",
    "    for text, annotations in data:\n",
    "        entities = sorted(annotations[\"entities\"], key=lambda x: x[0])  # Sort by start index\n",
    "        for i in range(len(entities) - 1):\n",
    "            start1, end1, label1 = entities[i]\n",
    "            start2, end2, label2 = entities[i + 1]\n",
    "            if end1 > start2:  # Overlapping condition\n",
    "                print(f\"Overlap detected between {label1} ({start1}-{end1}) and {label2} ({start2}-{end2})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb0ebdc6-c83a-4781-ac9a-740be390a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap detected between Skills (1136-1248) and Companies worked at (1209-1215)\n",
      "Overlap detected between Skills (1356-1793) and Companies worked at (1417-1423)\n",
      "Overlap detected between Companies worked at (34-50) and Companies worked at (34-49)\n",
      "Overlap detected between Skills (4121-4399) and Companies worked at (4186-4191)\n",
      "Overlap detected between College Name (2116-2143) and Companies worked at (2128-2144)\n",
      "Overlap detected between Skills (886-897) and Skills (894-897)\n",
      "Overlap detected between Degree (1106-1183) and College Name (1153-1184)\n",
      "Overlap detected between Skills (1734-1745) and Skills (1742-1745)\n",
      "Overlap detected between Skills (1748-1790) and Skills (1765-1771)\n",
      "Overlap detected between Skills (1794-1830) and Skills (1811-1815)\n",
      "Overlap detected between Skills (1844-1873) and Skills (1844-1860)\n",
      "Overlap detected between Skills (3466-3819) and Companies worked at (3535-3541)\n",
      "Overlap detected between Skills (6305-7258) and Companies worked at (6861-6870)\n",
      "Overlap detected between Skills (203-208) and Skills (203-207)\n",
      "Overlap detected between Skills (210-214) and Skills (210-213)\n",
      "Overlap detected between Designation (812-835) and Designation (819-835)\n",
      "Overlap detected between Skills (667-671) and Skills (667-670)\n",
      "Overlap detected between Skills (3243-3247) and Skills (3243-3246)\n",
      "Overlap detected between Skills (11578-11582) and Skills (11578-11581)\n",
      "Overlap detected between Skills (13883-13887) and Skills (13883-13886)\n",
      "Overlap detected between Skills (3345-3896) and Companies worked at (3385-3391)\n",
      "Overlap detected between Skills (1145-1274) and Companies worked at (1161-1164)\n",
      "Overlap detected between Skills (2861-3074) and Companies worked at (2886-2889)\n",
      "Overlap detected between Designation (13-40) and Designation (13-40)\n",
      "Overlap detected between Designation (1173-1200) and Designation (1180-1201)\n",
      "Overlap detected between Designation (15-45) and Designation (15-44)\n",
      "Overlap detected between Companies worked at (47-57) and Companies worked at (47-57)\n",
      "Overlap detected between Skills (4213-4349) and Companies worked at (4231-4238)\n",
      "Overlap detected between Designation (13-35) and Designation (13-34)\n",
      "Overlap detected between Designation (370-392) and Designation (370-391)\n",
      "Overlap detected between College Name (2528-2540) and College Name (2528-2532)\n",
      "Overlap detected between Designation (14-35) and Designation (21-35)\n",
      "Overlap detected between Designation (816-837) and Designation (823-837)\n",
      "Overlap detected between Skills (6646-7279) and Companies worked at (6682-6687)\n",
      "Overlap detected between Companies worked at (116-141) and Companies worked at (116-122)\n",
      "Overlap detected between Companies worked at (463-488) and Companies worked at (463-469)\n",
      "Overlap detected between Companies worked at (60-84) and Companies worked at (60-66)\n",
      "Overlap detected between Companies worked at (263-287) and Companies worked at (263-269)\n",
      "Overlap detected between Companies worked at (556-580) and Companies worked at (556-562)\n",
      "Overlap detected between Companies worked at (2184-2208) and Companies worked at (2184-2190)\n",
      "Overlap detected between Companies worked at (2407-2431) and Companies worked at (2407-2413)\n",
      "Overlap detected between Skills (2557-2606) and Skills (2562-2568)\n",
      "Overlap detected between Companies worked at (2665-2689) and Companies worked at (2665-2671)\n",
      "Overlap detected between Companies worked at (3132-3156) and Companies worked at (3132-3138)\n",
      "Overlap detected between Companies worked at (4374-4398) and Companies worked at (4374-4380)\n",
      "Overlap detected between Skills (415-1910) and Companies worked at (941-947)\n",
      "Overlap detected between Skills (3912-4040) and Companies worked at (3939-3948)\n",
      "Overlap detected between Skills (1862-1890) and Skills (1865-1868)\n",
      "Overlap detected between Skills (2058-2091) and Skills (2058-2074)\n",
      "Overlap detected between Skills (1311-1763) and Companies worked at (1342-1348)\n",
      "Overlap detected between College Name (4744-4789) and Location (4774-4778)\n",
      "Overlap detected between Skills (1416-3890) and Graduation Year (3461-3465)\n",
      "Overlap detected between Designation (15-50) and Designation (15-49)\n",
      "Overlap detected between Skills (7745-7837) and Years of Experience (7777-7784)\n",
      "Overlap detected between Designation (9-32) and Designation (10-33)\n",
      "Overlap detected between Designation (15-58) and Designation (15-34)\n",
      "Overlap detected between Degree (4681-4717) and Companies worked at (4708-4717)\n",
      "Overlap detected between Name (0-15) and Location (0-4)\n",
      "Overlap detected between Email Address (1563-1608) and Location (1576-1580)\n",
      "Overlap detected between College Name (729-775) and Location (729-735)\n",
      "Overlap detected between Skills (497-541) and Companies worked at (497-507)\n",
      "Overlap detected between College Name (295-321) and Companies worked at (295-305)\n",
      "Overlap detected between Skills (458-501) and Skills (458-467)\n",
      "Overlap detected between Skills (1502-1600) and Companies worked at (1514-1523)\n",
      "Overlap detected between Skills (367-519) and Companies worked at (381-391)\n",
      "Overlap detected between Name (0-8) and Location (3-8)\n",
      "Overlap detected between Email Address (51-90) and Location (67-72)\n",
      "Overlap detected between Skills (970-1002) and Companies worked at (983-992)\n",
      "Overlap detected between Designation (1438-1463) and Companies worked at (1438-1447)\n",
      "Overlap detected between Designation (1476-1501) and Companies worked at (1476-1485)\n",
      "Overlap detected between Skills (2537-2756) and Companies worked at (2537-2546)\n",
      "Overlap detected between Degree (8133-8136) and Degree (8133-8136)\n",
      "Overlap detected between Designation (2907-2938) and Designation (2907-2937)\n",
      "Overlap detected between Designation (17-58) and Companies worked at (42-50)\n",
      "Overlap detected between Designation (970-1011) and Companies worked at (995-1003)\n",
      "Overlap detected between Designation (1080-1121) and Companies worked at (1105-1113)\n",
      "Overlap detected between Companies worked at (38-55) and Location (46-55)\n",
      "Overlap detected between Skills (3380-3829) and Graduation Year (3803-3807)\n",
      "Overlap detected between Companies worked at (3973-3990) and Location (3981-3990)\n",
      "Overlap detected between Companies worked at (1258-1273) and Location (1258-1262)\n",
      "Overlap detected between Designation (16-58) and Companies worked at (16-25)\n",
      "Overlap detected between Designation (539-581) and Companies worked at (539-548)\n",
      "Overlap detected between Designation (22-56) and Designation (52-55)\n"
     ]
    }
   ],
   "source": [
    "check_overlapping_entities(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b10d3d49-5e03-4aea-a341-0f146dda128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "def clean_training_data(train_data):\n",
    "    \"\"\"Cleans training data by ensuring no overlapping entities and stripping spaces.\"\"\"\n",
    "    cleaned_data = []\n",
    "    for text, annotations in train_data:\n",
    "        entities = sorted(annotations[\"entities\"], key=lambda x: (x[0], x[1]))  # Sort entities by start position\n",
    "        has_overlap = False\n",
    "\n",
    "        # Check for overlaps\n",
    "        for i in range(len(entities) - 1):\n",
    "            start1, end1, label1 = entities[i]\n",
    "            start2, end2, label2 = entities[i + 1]\n",
    "\n",
    "            if start2 < end1:  # Overlap detected\n",
    "                has_overlap = True\n",
    "                break  # Skip this resume\n",
    "\n",
    "        if not has_overlap:\n",
    "            # Trim spaces in entity spans\n",
    "            new_entities = []\n",
    "            for start, end, label in entities:\n",
    "                trimmed_text = text[start:end].strip()\n",
    "                new_start = text.find(trimmed_text, start)  # Get new start position\n",
    "                new_end = new_start + len(trimmed_text)\n",
    "                new_entities.append((new_start, new_end, label))\n",
    "\n",
    "            cleaned_data.append((text, {\"entities\": new_entities}))  # Store cleaned entities\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "# Load blank Spacy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Clean the training data\n",
    "TRAIN_DATA_CLEANED = clean_training_data(TRAIN_DATA)\n",
    "print(len(TRAIN_DATA_CLEANED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95e969-da66-495c-8f3e-91817acc5a65",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d81d2d-5890-4175-9a42-43a8422f7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all datasets\n",
    "from dataset_1 import TRAIN_DATA_1\n",
    "from dataset_2 import TRAIN_DATA_2\n",
    "from dataset_3 import TRAIN_DATA_3\n",
    "from dataset_4 import TRAIN_DATA_4\n",
    "from dataset_5 import TRAIN_DATA_5\n",
    "from test_dataset_1 import test_1\n",
    "from test_dataset_2 import test_2\n",
    "# Merge all datasets into one\n",
    "TRAIN_DATA_MERGED = TRAIN_DATA_1 + TRAIN_DATA_2 + TRAIN_DATA_3 + TRAIN_DATA_4 + TRAIN_DATA_5 +test_1+test_2\n",
    "\n",
    "# Save the merged dataset to a new file\n",
    "with open(\"merged_dataset.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"TRAIN_DATA_MERGED = \" + str(TRAIN_DATA_MERGED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a5408e4c-fd33-4aa9-b2c5-15334a63777c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(224, 243, 'POSTE'), (245, 264, 'POSTE'), (3274, 3293, 'POSTE'), (3295, 3314, 'POSTE'), (6225, 6244, 'POSTE'), (6246, 6265, 'POSTE'), (13826, 13845, 'POSTE'), (13860, 13879, 'POSTE'), (27198, 27217, 'POSTE'), (27219, 27238, 'POSTE'), (28445, 28464, 'POSTE'), (28466, 28485, 'POSTE'), (28575, 28594, 'POSTE'), (30997, 31016, 'POSTE'), (31018, 31037, 'POSTE'), (38598, 38617, 'POSTE'), (38632, 38651, 'POSTE'), (51970, 51989, 'POSTE'), (51991, 52010, 'POSTE'), (53217, 53236, 'POSTE'), (53238, 53257, 'POSTE'), (53347, 53366, 'POSTE'), (55769, 55788, 'POSTE'), (55790, 55809, 'POSTE'), (63370, 63389, 'POSTE'), (63404, 63423, 'POSTE'), (76742, 76761, 'POSTE'), (76763, 76782, 'POSTE'), (77989, 78008, 'POSTE'), (78010, 78029, 'POSTE'), (78119, 78138, 'POSTE'), (80052, 80071, 'POSTE'), (80073, 80092, 'POSTE'), (83102, 83121, 'POSTE'), (83123, 83142, 'POSTE'), (86053, 86072, 'POSTE'), (86074, 86093, 'POSTE'), (93654, 93673, 'POSTE'), (93688, 93707, 'POSTE')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from merged_dataset import TRAIN_DATA_MERGED\n",
    "\n",
    "# Check a few examples\n",
    "for text, annotations in TRAIN_DATA_MERGED[:1]:  # Print only the first sample\n",
    "    entities = annotations.get(\"entities\", [])  # Extract entity list\n",
    "    print(entities)  # Print (start, end, label) tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a80c84fb-c8e5-40d8-b409-7b1ceeee297f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(TRAIN_DATA_MERGED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50c5e3e6-ba32-4372-8181-cd43fa6fe194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_md  # For French model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c630544-5234-4373-88e8-19f65452cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc8fff63-f4a8-4a3e-b3e7-67a5cc315471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_overlapping_entities(train_data):\n",
    "    overlapping_samples = []\n",
    "    total_overlaps = 0\n",
    "    texts_with_overlaps = 0\n",
    "    \n",
    "    for text, annotations in train_data:\n",
    "        entities = annotations[\"entities\"]\n",
    "        entities_sorted = sorted(entities, key=lambda x: x[0])  # Sort entities by their start position\n",
    "        \n",
    "        overlaps_in_text = 0\n",
    "        \n",
    "        # Check for overlaps\n",
    "        for i in range(len(entities_sorted) - 1):\n",
    "            start_1, end_1, label_1 = entities_sorted[i]\n",
    "            start_2, end_2, label_2 = entities_sorted[i + 1]\n",
    "            \n",
    "            if start_2 < end_1:  # Overlap detected\n",
    "                overlapping_samples.append({\n",
    "                    'text': text,\n",
    "                    'overlap': [(start_1, end_1, label_1), (start_2, end_2, label_2)],\n",
    "                    'positions': (start_1, end_1, start_2, end_2)\n",
    "                })\n",
    "                overlaps_in_text += 1\n",
    "        \n",
    "        # If overlaps are detected in this text, increase the counter\n",
    "        if overlaps_in_text > 0:\n",
    "            texts_with_overlaps += 1\n",
    "            total_overlaps += overlaps_in_text\n",
    "\n",
    "    return overlapping_samples, texts_with_overlaps, total_overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a328e10-49ad-4ee8-9ac9-bb415be883fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts with overlaps: 8\n",
      "Total number of overlaps: 449\n"
     ]
    }
   ],
   "source": [
    "# Call function to detect overlaps and count them\n",
    "overlaps, texts_with_overlaps, total_overlaps = detect_overlapping_entities(TRAIN_DATA_MERGED)\n",
    "\n",
    "# Output results\n",
    "print(f\"Number of texts with overlaps: {texts_with_overlaps}\")\n",
    "print(f\"Total number of overlaps: {total_overlaps}\")\n",
    "\n",
    "# If you want to print the overlapping samples, uncomment this:\n",
    "# for overlap in overlaps:\n",
    "#     print(f\"Overlap detected in: {overlap['text']}\")\n",
    "#     print(f\"Overlapping Entities: {overlap['overlap']}\")\n",
    "#     print(f\"Positions: {overlap['positions']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a6e5ab5-9385-4f18-b4a1-02dda2bece33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_overlap(entity1, entity2):\n",
    "    \"\"\"Check if two entity spans overlap\"\"\"\n",
    "    return not (entity1[1] <= entity2[0] or entity1[0] >= entity2[1])\n",
    "\n",
    "def fix_overlaps(data):\n",
    "    \"\"\"Remove or adjust overlapping entities\"\"\"\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for text, annotations in data:\n",
    "        entities = sorted(annotations[\"entities\"], key=lambda x: (x[0], x[1]))  # Sort by start position\n",
    "        non_overlapping_entities = []\n",
    "\n",
    "        for entity in entities:\n",
    "            if all(not has_overlap(entity, existing) for existing in non_overlapping_entities):\n",
    "                non_overlapping_entities.append(entity)\n",
    "\n",
    "        cleaned_data.append((text, {\"entities\": non_overlapping_entities}))\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e1b5851-fd37-4ef1-b118-91a351afd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix overlapping entities\n",
    "fixed_train_data = fix_overlaps(TRAIN_DATA_MERGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2929eb9e-8104-4fe6-96fa-2fe5cb982f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts with overlaps: 0\n",
      "Total number of overlaps: 0\n"
     ]
    }
   ],
   "source": [
    "# Call function to detect overlaps and count them\n",
    "overlaps, texts_with_overlaps, total_overlaps = detect_overlapping_entities(fixed_train_data)\n",
    "\n",
    "# Output results\n",
    "print(f\"Number of texts with overlaps: {texts_with_overlaps}\")\n",
    "print(f\"Total number of overlaps: {total_overlaps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e59de-734d-4e44-8ce8-4aa9ba6dae53",
   "metadata": {},
   "source": [
    "# Training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65c92acd-a55d-46b8-ae2a-f9515cd32fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \" Youssef CHLIH\n",
      " Étudiant en Intelligence Artificie...\" with entities \"[(57, 79, 'COMPETENCES'), (81, 106, 'CONTACT'), (1...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 87682.27471699925\n",
      "Epoch 1 Loss: 10407.436692103383\n",
      "Epoch 2 Loss: 7733.415592869509\n",
      "Epoch 3 Loss: 7017.307402287294\n",
      "Epoch 4 Loss: 6516.3333772274655\n",
      "Epoch 5 Loss: 7529.477312932133\n",
      "Epoch 6 Loss: 6226.654511233508\n",
      "Epoch 7 Loss: 5089.623256621429\n",
      "Epoch 8 Loss: 6297.587068647356\n",
      "Epoch 9 Loss: 4355.913330746995\n",
      "Epoch 10 Loss: 3841.469888915218\n",
      "Epoch 11 Loss: 3761.360140399487\n",
      "Epoch 12 Loss: 3460.1770082560483\n",
      "Epoch 13 Loss: 2713.702052448542\n",
      "Epoch 14 Loss: 2338.266921237242\n",
      "Epoch 15 Loss: 2512.3213707148266\n",
      "Epoch 16 Loss: 2185.789734778288\n",
      "Epoch 17 Loss: 2473.3136353169366\n",
      "Epoch 18 Loss: 2115.1534215477404\n",
      "Epoch 19 Loss: 2025.4219211069874\n",
      "Epoch 20 Loss: 1975.3808352535443\n",
      "Epoch 21 Loss: 1911.032777137726\n",
      "Epoch 22 Loss: 1718.755414368\n",
      "Epoch 23 Loss: 1819.2172411206334\n",
      "Epoch 24 Loss: 1984.9557808629404\n",
      "Epoch 25 Loss: 1805.861596494536\n",
      "Epoch 26 Loss: 1887.4548255835\n",
      "Epoch 27 Loss: 1723.4438220222096\n",
      "Early stopping at epoch 27.\n"
     ]
    }
   ],
   "source": [
    "# Load blank Spacy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER model\n",
    "for _, annotations in fixed_train_data:\n",
    "    for start, end, label in annotations[\"entities\"]:\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Train the model\n",
    "optimizer = nlp.begin_training()\n",
    "nlp.config[\"training\"][\"optimizer\"][\"learn_rate\"] = 0.0001  \n",
    "\n",
    "best_loss = float('inf')  \n",
    "patience_counter = 0  \n",
    "patience_threshold = 5 \n",
    "\n",
    "# List to store the losses  \n",
    "losses_list = []  \n",
    "\n",
    "for epoch in range(50):  # Adjust epochs as needed\n",
    "    losses = {}\n",
    "    for text, annotations in fixed_train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "       \n",
    "        nlp.update([example], drop=0.001, losses=losses)\n",
    "\n",
    "    # Store the loss for the current epoch  \n",
    "    losses_list.append(losses['ner'])\n",
    "    print(f\"Epoch {epoch} Loss: {losses['ner']}\")\n",
    "\n",
    "    # Early stopping condition  \n",
    "    if losses['ner'] < best_loss:  \n",
    "        best_loss = losses['ner']  \n",
    "        patience_counter = 0  \n",
    "    else:  \n",
    "        patience_counter += 1  \n",
    "    \n",
    "    if patience_counter >= patience_threshold:  \n",
    "        print(f\"Early stopping at epoch {epoch}.\")  \n",
    "        break \n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(\"custom_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9a623-7f7a-4e56-a2e9-e4f7ada74dc7",
   "metadata": {},
   "source": [
    "# Test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c43f0f77-5631-40d8-8ecf-a66e466ae804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "nlp_ner = spacy.load(\"custom_ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89f19858-ab35-4643-aea7-61759c927cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all test datasets\n",
    "from test_dataset_3 import test_3\n",
    "\n",
    "\n",
    "# Merge all datasets into one\n",
    "TEST_DATA_MERGED = test_3\n",
    "\n",
    "# Save the merged dataset to a new file\n",
    "with open(\"merged_TEST_dataset.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"TEST_DATA_MERGED = \" + str(TEST_DATA_MERGED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eae5ff19-fdf4-4faa-bb08-65b184968099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from merged_TEST_dataset import TEST_DATA_MERGED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027ab64-0811-4f48-b4d2-eb9bb015495f",
   "metadata": {},
   "source": [
    "# Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66865b50-1a26-45b1-b443-05a49e3e70b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts with overlaps: 12\n",
      "Total number of overlaps: 238\n"
     ]
    }
   ],
   "source": [
    "# Call function to detect overlaps and count them\n",
    "overlaps, texts_with_overlaps, total_overlaps = detect_overlapping_entities(TEST_DATA_MERGED)\n",
    "\n",
    "# Output results\n",
    "print(f\"Number of texts with overlaps: {texts_with_overlaps}\")\n",
    "print(f\"Total number of overlaps: {total_overlaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73833704-3733-416a-a328-5c15b627cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix overlapping entities\n",
    "fixed_test_data = fix_overlaps(TEST_DATA_MERGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60d9a8be-7054-4ed6-ba56-ed36adadfad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts with overlaps: 0\n",
      "Total number of overlaps: 0\n"
     ]
    }
   ],
   "source": [
    "# Call function to detect overlaps and count them\n",
    "overlaps, texts_with_overlaps, total_overlaps = detect_overlapping_entities(fixed_test_data)\n",
    "\n",
    "# Output results\n",
    "print(f\"Number of texts with overlaps: {texts_with_overlaps}\")\n",
    "print(f\"Total number of overlaps: {total_overlaps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abe689ac-5a93-4e86-93fd-39ff5090788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(fixed_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45b9fbb3-1aca-4bcf-82d9-6f0eeda5d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate_ner(nlp, test_data):\n",
    "    \"\"\"Evaluate a spaCy NER model using proper entity alignment.\"\"\"\n",
    "    scorer = Scorer()\n",
    "    examples = []\n",
    "    for text, annotations in test_data:\n",
    "        # Create a Doc object for the text and apply the model's predictions\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        example.predicted = nlp(text)  # Process with the model\n",
    "        examples.append(example)\n",
    "    \n",
    "    # Calculate scores using spaCy's scorer\n",
    "    scores = scorer.score(examples)\n",
    "    return {\n",
    "        \"precision\": scores[\"ents_p\"],\n",
    "        \"recall\": scores[\"ents_r\"],\n",
    "        \"f1_score\": scores[\"ents_f\"],\n",
    "        \"details\": scores[\"ents_per_type\"]  # Metrics per entity type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "528c0a26-3ee7-4f77-a677-e8c80c0a3147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Precision: 0.23\n",
      "Overall Recall: 0.09\n",
      "Overall F1: 0.13\n"
     ]
    }
   ],
   "source": [
    "# Load your trained NER model  \n",
    "nlp_ner = spacy.load(\"custom_ner_model\")\n",
    "\n",
    "# Sample test data (replace with your actual test data)  \n",
    "test_data = fixed_test_data \n",
    "\n",
    "metrics = evaluate_ner(nlp, test_data)\n",
    "print(f\"Overall Precision: {metrics['precision']:.2f}\")\n",
    "print(f\"Overall Recall: {metrics['recall']:.2f}\")\n",
    "print(f\"Overall F1: {metrics['f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d362dcf-f5d3-4167-841d-7c7fc704f068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8607\n",
      "Recall: 0.9181\n",
      "F1 Score: 0.8885\n"
     ]
    }
   ],
   "source": [
    "# Define your validation dataset (format: text, {\"entities\": [(start, end, \"LABEL\")]})\n",
    "VALIDATION_DATA = fixed_train_data[2:6]\n",
    "\n",
    "# Convert the validation data into spaCy's Example format\n",
    "examples = []\n",
    "for text, annotations in VALIDATION_DATA:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    examples.append(example)\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "metrics = nlp.evaluate(examples)\n",
    "\n",
    "# Print the evaluation metrics: Precision, Recall, F1 score\n",
    "print(f\"Precision: {metrics['ents_p']:.4f}\")\n",
    "print(f\"Recall: {metrics['ents_r']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['ents_f']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efc771-8f1c-4fb2-93ca-a353aba2520c",
   "metadata": {},
   "source": [
    "# Extracting from PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e638ca4-0493-4874-9d18-d84085c028bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.1/2.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 10.2 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install pymupdf \n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "784305f2-b64d-4218-8dd7-ef989fe585a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langages de Programmation : Python, Java , CSS, HTML, PHP , JavaScript .\n",
      " Framework de Développement :JavaEE(Java Enterprise Edition) , Spring Boot. \n",
      "Bases de Données : MySQL , NoSQL  . \n",
      "Intelligence Artificielle et Big Data : Machine Learning , Deep Learning ,Traitement du\n",
      "Langage Naturel (NLP) , Vision par Ordinateur , Big Data ( hadoop , Apache Pig ) . \n",
      "DevOps & DataOps .\n",
      "GHIZLANE AIT LMOUDDEN\n",
      "PROFIL\n",
      "En tant qu'étudiante en deuxième année d'intelligence artificielle et d'ingénierie des données à\n",
      "l'École Supérieure de Technologie de Nador, je suis activement à la recherche d'un stage de fin\n",
      "d'études d'une durée de deux mois , à partir du mois d'avril. \n",
      "Ce stage représente pour moi une opportunité précieuse de mettre en pratique mes\n",
      "connaissances académiques tout en acquérant une expérience professionnelle significative. \n",
      " EXPÉRIENCE PROFESSIONNELLE \n",
      " FORMATIONS \n",
      "    2ᵉ année en Intelligence Artificielle et Ingénierie des Données \n",
      "    École Supérieure de Technologie de Nador.\n",
      "  2023  -  2025\n",
      "Développement d'un système de détection des sentiments par vision par ordinateur\n",
      "atteignant 70 % de précision, renforçant mes compétences en machine learning et\n",
      "traitement d'images.\n",
      "Réalisation d'une application de gestion des patients hospitaliers en utilisant le framework\n",
      "Spring\n",
      "Arabe : Maternelle\n",
      "Français : Lu, Parlé Et Écrit\n",
      "Anglais : Lu , Parlé Et Écrit\n",
      "COMPÉTENCES ACADÉMIQUES\n",
      "JUILLET 2024\n",
      " Stage d'Initiation en Service IT  / Concentrix\n",
      "Réalisation d'un projet d'optimisation de la performance d'un centre d'appel en utilisant des\n",
      "techniques de machine learning.\n",
      "Découverte et implication dans le fonctionnement du service IT : gestion des incidents,\n",
      "résolution de problèmes techniques et support aux utilisateurs.\n",
      "     Baccalauréat Sciences Physiques & Chimiques \n",
      "   Lycée Abdlkrim El Khattabi\n",
      " 2022  -  2023\n",
      "Nador  • +212 654382904 • aitlmouddenghiz@gmail.com\n",
      "À la recherche d'un stage de fin d'études\n",
      " PROJETS ACADÉMIQUES\n",
      "COMPÉTENCES PERSONNELLES  \n",
      "LANGUES \n",
      "Sens de Responsabilité \n",
      "Capacité d'Adaptation \n",
      "Travail en Équipe\n",
      "Gestion du Temps\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_path = [\"CV_GHIZLANE AIT LMOUDDEN.pdf\",\"CV_GHIZLANE AIT LMOUDDEN.pdf\"] # Replace with the actual PDF file path\n",
    "extracted_text_list = []\n",
    "for cv in pdf_path:\n",
    "    \n",
    "    cv_text = extract_text_from_pdf(cv)\n",
    "    extracted_text_list.append(cv_text)\n",
    "\n",
    "print(cv_text)  # Print extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee600d-73a9-4328-bce3-46afdd4b8b41",
   "metadata": {},
   "source": [
    "# Ranking CVs based on the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d19f9de3-da71-494c-bcd7-0ee788d008a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8ff43f13-2302-4b0a-a419-dcb115da719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained SpaCy NER model\n",
    "nlp = spacy.load(\"custom_ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ef2aaca-2e1b-4526-833e-d55df3b506a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CVs import cvs_list as cvs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c8a8339-37e3-4d6a-a029-250d71bff690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Job posting:\n",
    "job_text = \"\"\"Date\n",
    "31-07-2024\n",
    "Direction\n",
    "JUNIOR DATA SCIENTIST\n",
    "Contrat\n",
    "Localisation\n",
    "Casablanca\n",
    "Fonction\n",
    "Autre fonction\n",
    "Contexte du recrutement et définition de poste\n",
    "We are looking for a Junior Data Scientist. They are expected to conduct in-depth predictive analyses using complex data to business units make informed decisions. Using statistical modeling and machine learning techniques, the Data Scientist will create predictive models and propose corresponding metrics in order to evaluate the efficiency of these models.\n",
    "\n",
    "Key Responsibilities:\n",
    "\n",
    "Workflow Integration:\n",
    "Prepare and integrate developments into existing workflows.\n",
    "Comprehensive Documentation:\n",
    "Thoroughly document critical parts of the code.\n",
    "Document global workflows and the significance and use of data.\n",
    "Data Evaluation and Communication:\n",
    "Evaluate data quality before usage.\n",
    "Communicate date-related issues to the data management.\n",
    "Coding Standards and Documentation:\n",
    "Adhere to coding standards, stylistic conventions, and provided guidelines.\n",
    "Critically evaluate and propose improvements to existing code base.\n",
    "Prediction and Model Evaluation:\n",
    "Help define business metrics to evaluate data prediction outputs.\n",
    "Define technical metrics to evaluate model training.\n",
    "Technical Procedures:\n",
    "Document and model technical procedures specific to Data Science.\n",
    "Profil recherché\n",
    "Bachelor or Masters degree in Data Science, Computer Science, or a related field.\n",
    "Knowledge in machine learning libraries (pandas, numpy, scikit-learn, pytorch, tensorflow and others).\n",
    "Knowledge in Python programming language.\n",
    "Understanding of machine learning models and data preprocessing.\n",
    "Strong analytical and problem-solving skills.\n",
    "Ability to work in a team.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cbaa259f-f5e7-48e6-b55e-793ee8f37cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_entities(text):\n",
    "    \"\"\"Extract unique entities from text using the trained NER model.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = set()  # Store values as sets to ensure uniqueness\n",
    "        entities[ent.label_].add(ent.text.lower())  # Store lowercase for consistency\n",
    "    \n",
    "    # Convert sets back to lists\n",
    "    return {key: list(values) for key, values in entities.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e278c5a5-006e-4d6b-8a2a-216c108ea61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_cvs(cv_texts):\n",
    "    \"\"\"Runs the NER model on a list of CV texts and returns the entities extracted.\"\"\"\n",
    "    cv_entities_list = []\n",
    "    \n",
    "    for cv_text in cv_texts:\n",
    "        # Extract unique entities for each CV\n",
    "        cv_entities = extract_unique_entities(cv_text)\n",
    "        cv_entities_list.append(cv_entities)\n",
    "    \n",
    "    return cv_entities_list\n",
    "\n",
    "def extract_unique_entities_for_all_cvs(cv_entities_list):\n",
    "    \"\"\"Extract unique entities from all CVs into a list.\"\"\"\n",
    "    unique_entities_list = []\n",
    "    for cv_entities in cv_entities_list:\n",
    "        unique_entities = {key: list(set(values)) for key, values in cv_entities.items()}  # Ensure uniqueness\n",
    "        unique_entities_list.append(unique_entities)\n",
    "    \n",
    "    return unique_entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "11a32f65-4524-4d6c-a7b2-927eb4e3bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(cv_entities, job_entities, keyword_weights):\n",
    "    \"\"\"Computes similarity between CV entities and job description entities using TF-IDF & weighted keywords.\"\"\"\n",
    "    cv_text = \" \".join([\" \".join(values) for values in cv_entities.values()])\n",
    "    job_text = \" \".join([\" \".join(values) for values in job_entities.values()])\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([cv_text, job_text])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "    \n",
    "    # Apply recruiter-defined weights\n",
    "    weighted_score = 0\n",
    "    for keyword, weight in keyword_weights.items():\n",
    "        if any(keyword in \" \".join(cv_entities.get(k, [])) for k in cv_entities):\n",
    "            weighted_score += weight\n",
    "    \n",
    "    final_score = similarity_score * weighted_score\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fab6739f-cef0-4f2e-a49a-9e4e9fe0450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_cvs_by_similarity(cv_entities_list, job_entities, keyword_weights):\n",
    "    \"\"\"Compute similarity between each CV and the job posting and rank them.\"\"\"\n",
    "    ranked_cvs = []\n",
    "    \n",
    "    for i, cv_entities in enumerate(cv_entities_list):\n",
    "        similarity_score = compute_similarity(cv_entities, job_entities, keyword_weights)\n",
    "        ranked_cvs.append({\"name\": f\"CV {i+1}\", \"score\": similarity_score, \"entities\": cv_entities})\n",
    "    \n",
    "    # Rank CVs by similarity score (descending order)\n",
    "    ranked_cvs.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    return ranked_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3217bca7-8d27-4d07-9be6-1865c71df6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example recruiter-defined weights\n",
    "keyword_weights = {\"python\": 0.8, \"java\": 0.4, \"baccalaureate\": 0.6}\n",
    "\n",
    "# Example job posting\n",
    "job_entities = extract_unique_entities(job_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a0821882-fc5d-4355-8516-8c81449f5c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. CV 2 - Similarity Score: 0.4068\n",
      "2. CV 1 - Similarity Score: 0.1584\n"
     ]
    }
   ],
   "source": [
    "# 1. Run model on CVs and extract entities\n",
    "cv_entities_list = run_model_on_cvs(cvs_text)\n",
    "\n",
    "# 2. Ensure entities in each CV are unique\n",
    "unique_cv_entities_list = extract_unique_entities_for_all_cvs(cv_entities_list)\n",
    "\n",
    "# 3. Rank the CVs based on similarity with job posting\n",
    "ranked_cvs = rank_cvs_by_similarity(unique_cv_entities_list, job_entities, keyword_weights)\n",
    "\n",
    "# Print ranked CVs\n",
    "for rank, cv in enumerate(ranked_cvs, 1):\n",
    "    print(f\"{rank}. {cv['name']} - Similarity Score: {cv['score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
